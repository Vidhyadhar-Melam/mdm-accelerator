2026-02-10 18:02:27,619 [INFO] Starting deduplication job...
2026-02-10 18:02:40,839 [INFO] Loaded raw data for CRM from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-10 18:03:24,321 [INFO] CRM deduplication complete. Clean=5, Conflicted=11
2026-02-10 18:03:24,603 [INFO] Loaded raw data for ERP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-10 18:03:51,994 [INFO] ERP deduplication complete. Clean=17, Conflicted=0
2026-02-10 18:03:52,275 [INFO] Loaded raw data for Salesforce from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-10 18:04:15,823 [INFO] Salesforce deduplication complete. Clean=10, Conflicted=7
2026-02-10 18:04:16,244 [INFO] Loaded raw data for SAP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-10 18:04:41,283 [INFO] SAP deduplication complete. Clean=17, Conflicted=0
2026-02-10 18:05:01,157 [INFO] Deduplication complete. Run ID=b68ff075-d118-4199-b759-7b224e9b6ac1, Clean=49, Conflicted=18, Duration=133.66s
2026-02-10 18:05:05,354 [INFO] Closing down clientserver connection
2026-02-11 10:30:35,094 [INFO] Starting deduplication job...
2026-02-11 10:30:51,315 [INFO] Loaded raw data for CRM from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 10:31:31,083 [INFO] CRM deduplication complete. Clean=5, Conflicted=11
2026-02-11 10:31:36,525 [INFO] Updated checkpoint for CRM with 2026-02-11 10:27:53.048000
2026-02-11 10:31:36,806 [INFO] Loaded raw data for ERP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 10:31:54,722 [INFO] ERP deduplication complete. Clean=17, Conflicted=0
2026-02-11 10:31:57,414 [INFO] Updated checkpoint for ERP with 2026-02-11 10:28:43.074000
2026-02-11 10:31:57,797 [INFO] Loaded raw data for Salesforce from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 10:32:14,065 [INFO] Salesforce deduplication complete. Clean=10, Conflicted=7
2026-02-11 10:32:15,680 [INFO] Updated checkpoint for Salesforce with 2026-02-11 10:29:04.644000
2026-02-11 10:32:15,977 [INFO] Loaded raw data for SAP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 10:32:32,577 [INFO] SAP deduplication complete. Clean=17, Conflicted=0
2026-02-11 10:32:35,717 [INFO] Updated checkpoint for SAP with 2026-02-11 10:29:19.609000
2026-02-11 10:32:37,250 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 178, in main
    audit_df.write.format("delta").mode("append").save(str(PROJECT_ROOT / paths["audit_runs"]))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 1398, in save
    self._jwrite.save(path)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: 834d4991-1d1d-487f-ad59-a0ddfc45aba7).
To enable schema migration using DataFrameWriter or DataStreamWriter, please set:
'.option("mergeSchema", "true")'.
For other operations, set the session configuration
spark.databricks.delta.schema.autoMerge.enabled to "true". See the documentation
specific to the operation for details.

Table schema:
root
-- run_id: string (nullable = true)
-- job_name: string (nullable = true)
-- start_time: timestamp (nullable = true)
-- end_time: timestamp (nullable = true)
-- duration: double (nullable = true)
-- records_loaded: long (nullable = true)
-- records_rejected: long (nullable = true)
-- environment: string (nullable = true)


Data schema:
root
-- run_id: string (nullable = true)
-- job_name: string (nullable = true)
-- start_time: timestamp (nullable = true)
-- end_time: timestamp (nullable = true)
-- duration: double (nullable = true)
-- records_loaded: long (nullable = true)
-- records_rejected: long (nullable = true)
-- environment: string (nullable = true)
-- failed_sources: string (nullable = true)

         
2026-02-11 10:32:39,705 [INFO] Closing down clientserver connection
2026-02-11 10:55:32,573 [INFO] Starting deduplication job...
2026-02-11 10:55:52,892 [INFO] Loaded raw data for CRM from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 10:55:52,956 [INFO] Applied incremental filter for CRM using checkpoint 2026-02-11 10:27:53.048000
2026-02-11 10:56:30,647 [INFO] CRM deduplication complete. Clean=0, Conflicted=0
2026-02-11 10:56:32,670 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 180, in main
    audit_df = mask_sensitive_fields(audit_df, sources[0]["sensitive_fields"])
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 78, in mask_sensitive_fields
    df = df.withColumn(field, regexp_replace(col(field), r"(^[^@]{3})[^@]*(@.*$)", r"\1***\2"))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\dataframe.py", line 4789, in withColumn
    return DataFrame(self._jdf.withColumn(colName, col._jc), self.sparkSession)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `email` cannot be resolved. Did you mean one of the following? [`run_id`, `duration`, `end_time`, `job_name`, `start_time`].;
'Project [run_id#2329, job_name#2330, start_time#2331, end_time#2332, duration#2333, records_clean#2334L, records_conflicted#2335L, environment#2336, failed_sources#2337, regexp_replace('email, (^[^@]{3})[^@]*(@.*$), \1***\2, 1) AS email#2347]
+- LogicalRDD [run_id#2329, job_name#2330, start_time#2331, end_time#2332, duration#2333, records_clean#2334L, records_conflicted#2335L, environment#2336, failed_sources#2337], false

2026-02-11 10:56:34,236 [INFO] Closing down clientserver connection
2026-02-11 11:09:50,790 [INFO] Starting deduplication job...
2026-02-11 11:10:09,927 [INFO] Loaded raw data for CRM from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 11:10:10,052 [INFO] Applied incremental filter for CRM using checkpoint 2026-02-11 10:27:53.048000
2026-02-11 11:10:51,372 [INFO] CRM deduplication complete. Clean=0, Conflicted=0
2026-02-11 11:10:54,572 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 179, in main
    audit_df.write.format("delta").mode("append").save(str(PROJECT_ROOT / paths["audit_runs"]))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 1398, in save
    self._jwrite.save(path)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: 834d4991-1d1d-487f-ad59-a0ddfc45aba7).
To enable schema migration using DataFrameWriter or DataStreamWriter, please set:
'.option("mergeSchema", "true")'.
For other operations, set the session configuration
spark.databricks.delta.schema.autoMerge.enabled to "true". See the documentation
specific to the operation for details.

Table schema:
root
-- run_id: string (nullable = true)
-- job_name: string (nullable = true)
-- start_time: timestamp (nullable = true)
-- end_time: timestamp (nullable = true)
-- duration: double (nullable = true)
-- records_loaded: long (nullable = true)
-- records_rejected: long (nullable = true)
-- environment: string (nullable = true)


Data schema:
root
-- run_id: string (nullable = true)
-- job_name: string (nullable = true)
-- start_time: timestamp (nullable = true)
-- end_time: timestamp (nullable = true)
-- duration: double (nullable = true)
-- records_clean: long (nullable = true)
-- records_conflicted: long (nullable = true)
-- environment: string (nullable = true)
-- failed_sources: string (nullable = true)

         
2026-02-11 11:10:55,922 [INFO] Closing down clientserver connection
2026-02-11 11:20:35,430 [INFO] Starting deduplication job...
2026-02-11 11:21:03,368 [INFO] Loaded raw data for CRM from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 11:21:03,448 [INFO] Applied incremental filter for CRM using checkpoint 2026-02-11 10:27:53.048000
2026-02-11 11:21:41,848 [INFO] CRM deduplication complete. Clean=0, Conflicted=0
2026-02-11 11:21:43,099 [INFO] Loaded raw data for ERP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 11:21:43,116 [INFO] Applied incremental filter for ERP using checkpoint 2026-02-11 10:28:43.074000
2026-02-11 11:22:01,027 [INFO] ERP deduplication complete. Clean=0, Conflicted=0
2026-02-11 11:22:02,437 [INFO] Loaded raw data for Salesforce from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 11:22:02,452 [INFO] Applied incremental filter for Salesforce using checkpoint 2026-02-11 10:29:04.644000
2026-02-11 11:22:17,003 [INFO] Salesforce deduplication complete. Clean=0, Conflicted=0
2026-02-11 11:22:19,333 [INFO] Loaded raw data for SAP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 11:22:19,348 [INFO] Applied incremental filter for SAP using checkpoint 2026-02-11 10:29:19.609000
2026-02-11 11:22:32,900 [INFO] SAP deduplication complete. Clean=0, Conflicted=0
2026-02-11 11:22:37,466 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 179, in main
    audit_df.write.format("delta").mode("append").save(str(PROJECT_ROOT / paths["audit_runs"]))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 1398, in save
    self._jwrite.save(path)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: 834d4991-1d1d-487f-ad59-a0ddfc45aba7).
To enable schema migration using DataFrameWriter or DataStreamWriter, please set:
'.option("mergeSchema", "true")'.
For other operations, set the session configuration
spark.databricks.delta.schema.autoMerge.enabled to "true". See the documentation
specific to the operation for details.

Table schema:
root
-- run_id: string (nullable = true)
-- job_name: string (nullable = true)
-- start_time: timestamp (nullable = true)
-- end_time: timestamp (nullable = true)
-- duration: double (nullable = true)
-- records_loaded: long (nullable = true)
-- records_rejected: long (nullable = true)
-- environment: string (nullable = true)


Data schema:
root
-- run_id: string (nullable = true)
-- job_name: string (nullable = true)
-- start_time: timestamp (nullable = true)
-- end_time: timestamp (nullable = true)
-- duration: double (nullable = true)
-- records_clean: long (nullable = true)
-- records_conflicted: long (nullable = true)
-- environment: string (nullable = true)
-- failed_sources: string (nullable = true)

         
2026-02-11 11:22:39,114 [INFO] Closing down clientserver connection
2026-02-11 11:47:28,025 [INFO] Starting deduplication job...
2026-02-11 11:47:54,193 [INFO] Loaded raw data for CRM from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 11:48:25,194 [INFO] CRM deduplication complete. Clean=5, Conflicted=11
2026-02-11 11:48:28,310 [INFO] Updated checkpoint for CRM with 2026-02-11 10:27:53.048000
2026-02-11 11:48:28,623 [INFO] Loaded raw data for ERP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 11:48:46,648 [INFO] ERP deduplication complete. Clean=17, Conflicted=0
2026-02-11 11:48:48,672 [INFO] Updated checkpoint for ERP with 2026-02-11 10:28:43.074000
2026-02-11 11:48:48,984 [INFO] Loaded raw data for Salesforce from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 11:49:08,312 [INFO] Salesforce deduplication complete. Clean=10, Conflicted=7
2026-02-11 11:49:11,025 [INFO] Updated checkpoint for Salesforce with 2026-02-11 10:29:04.644000
2026-02-11 11:49:11,309 [INFO] Loaded raw data for SAP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 11:49:36,330 [INFO] SAP deduplication complete. Clean=17, Conflicted=0
2026-02-11 11:49:38,956 [INFO] Updated checkpoint for SAP with 2026-02-11 10:29:19.609000
2026-02-11 11:49:42,745 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 179, in main
    audit_df.write.format("delta").mode("append").save(str(PROJECT_ROOT / paths["audit_runs"]))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 1398, in save
    self._jwrite.save(path)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: 834d4991-1d1d-487f-ad59-a0ddfc45aba7).
To enable schema migration using DataFrameWriter or DataStreamWriter, please set:
'.option("mergeSchema", "true")'.
For other operations, set the session configuration
spark.databricks.delta.schema.autoMerge.enabled to "true". See the documentation
specific to the operation for details.

Table schema:
root
-- run_id: string (nullable = true)
-- job_name: string (nullable = true)
-- start_time: timestamp (nullable = true)
-- end_time: timestamp (nullable = true)
-- duration: double (nullable = true)
-- records_loaded: long (nullable = true)
-- records_rejected: long (nullable = true)
-- environment: string (nullable = true)


Data schema:
root
-- run_id: string (nullable = true)
-- job_name: string (nullable = true)
-- start_time: timestamp (nullable = true)
-- end_time: timestamp (nullable = true)
-- duration: double (nullable = true)
-- records_clean: long (nullable = true)
-- records_conflicted: long (nullable = true)
-- environment: string (nullable = true)
-- failed_sources: string (nullable = true)

         
2026-02-11 11:49:45,469 [INFO] Closing down clientserver connection
2026-02-11 12:15:20,006 [INFO] Starting deduplication job...
2026-02-11 12:15:34,189 [INFO] Loaded raw data for CRM from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 12:15:34,235 [INFO] Applied incremental filter for CRM using checkpoint 2026-02-11 10:27:53.048000
2026-02-11 12:16:16,746 [INFO] CRM deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:16:17,718 [INFO] Loaded raw data for ERP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 12:16:17,734 [INFO] Applied incremental filter for ERP using checkpoint 2026-02-11 10:28:43.074000
2026-02-11 12:16:38,769 [INFO] ERP deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:16:40,697 [INFO] Loaded raw data for Salesforce from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 12:16:40,697 [INFO] Applied incremental filter for Salesforce using checkpoint 2026-02-11 10:29:04.644000
2026-02-11 12:16:57,401 [INFO] Salesforce deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:16:59,214 [INFO] Loaded raw data for SAP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 12:16:59,230 [INFO] Applied incremental filter for SAP using checkpoint 2026-02-11 10:29:19.609000
2026-02-11 12:17:15,564 [INFO] SAP deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:17:17,612 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 184, in main
    audit_df.write.format("delta").mode("append").save(str(PROJECT_ROOT / paths["audit_runs"]))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 1398, in save
    self._jwrite.save(path)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: 834d4991-1d1d-487f-ad59-a0ddfc45aba7).
To enable schema migration using DataFrameWriter or DataStreamWriter, please set:
'.option("mergeSchema", "true")'.
For other operations, set the session configuration
spark.databricks.delta.schema.autoMerge.enabled to "true". See the documentation
specific to the operation for details.

Table schema:
root
-- run_id: string (nullable = true)
-- job_name: string (nullable = true)
-- start_time: timestamp (nullable = true)
-- end_time: timestamp (nullable = true)
-- duration: double (nullable = true)
-- records_loaded: long (nullable = true)
-- records_rejected: long (nullable = true)
-- environment: string (nullable = true)


Data schema:
root
-- run_id: string (nullable = true)
-- job_name: string (nullable = true)
-- start_time: timestamp (nullable = true)
-- end_time: timestamp (nullable = true)
-- duration: double (nullable = true)
-- records_clean: long (nullable = true)
-- records_conflicted: long (nullable = true)
-- environment: string (nullable = true)
-- failed_sources: string (nullable = true)

         
2026-02-11 12:17:20,004 [INFO] Closing down clientserver connection
2026-02-11 12:31:01,081 [INFO] Starting deduplication job...
2026-02-11 12:31:11,804 [INFO] Loaded raw data for CRM from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 12:31:11,867 [INFO] Applied incremental filter for CRM using checkpoint 2026-02-11 10:27:53.048000
2026-02-11 12:31:40,694 [INFO] CRM deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:31:41,437 [INFO] Loaded raw data for ERP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 12:31:41,452 [INFO] Applied incremental filter for ERP using checkpoint 2026-02-11 10:28:43.074000
2026-02-11 12:31:57,901 [INFO] ERP deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:31:59,046 [INFO] Loaded raw data for Salesforce from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 12:31:59,063 [INFO] Applied incremental filter for Salesforce using checkpoint 2026-02-11 10:29:04.644000
2026-02-11 12:32:15,432 [INFO] Salesforce deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:32:16,154 [INFO] Loaded raw data for SAP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 12:32:16,154 [INFO] Applied incremental filter for SAP using checkpoint 2026-02-11 10:29:19.609000
2026-02-11 12:32:27,668 [INFO] SAP deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:32:36,421 [INFO] Deduplication complete. Run ID=5fa5c453-16d8-48ed-bc63-530c4d84969f, Clean=0, Conflicted=0, Duration=87.10s
2026-02-11 12:32:38,039 [INFO] Closing down clientserver connection
2026-02-11 12:38:26,266 [INFO] Starting deduplication job...
2026-02-11 12:38:38,859 [INFO] Loaded raw data for CRM from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 12:38:38,927 [INFO] Applied incremental filter for CRM using checkpoint 2026-02-11 10:27:53.048000
2026-02-11 12:39:03,945 [INFO] CRM deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:39:04,665 [INFO] Loaded raw data for ERP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 12:39:04,680 [INFO] Applied incremental filter for ERP using checkpoint 2026-02-11 10:28:43.074000
2026-02-11 12:39:21,722 [INFO] ERP deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:39:22,475 [INFO] Loaded raw data for Salesforce from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 12:39:22,490 [INFO] Applied incremental filter for Salesforce using checkpoint 2026-02-11 10:29:04.644000
2026-02-11 12:39:38,345 [INFO] Salesforce deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:39:38,934 [INFO] Loaded raw data for SAP from C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 12:39:38,934 [INFO] Applied incremental filter for SAP using checkpoint 2026-02-11 10:29:19.609000
2026-02-11 12:39:51,293 [INFO] SAP deduplication complete. Clean=0, Conflicted=0
2026-02-11 12:39:59,273 [INFO] Deduplication complete. Run ID=c00a98b2-d11a-4054-a2d4-de093f97f884, Clean=0, Conflicted=0, Duration=85.57s
2026-02-11 12:40:00,433 [INFO] Closing down clientserver connection
2026-02-11 13:24:54,162 [INFO] Starting deduplication job in batch mode...
2026-02-11 13:24:54,287 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 118, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:24:54,381 [ERROR] Source CRM failed during deduplication
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 190, in main
    clean, conflicted = dedupe_source_batch(src, run_id)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 118, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:24:54,537 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 118, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:24:54,617 [ERROR] Source ERP failed during deduplication
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 190, in main
    clean, conflicted = dedupe_source_batch(src, run_id)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 118, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:24:54,748 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 118, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:24:54,816 [ERROR] Source Salesforce failed during deduplication
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 190, in main
    clean, conflicted = dedupe_source_batch(src, run_id)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 118, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:24:54,919 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 118, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:24:54,981 [ERROR] Source SAP failed during deduplication
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 190, in main
    clean, conflicted = dedupe_source_batch(src, run_id)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 118, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:24:55,150 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 208, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:24:55,769 [INFO] Closing down clientserver connection
2026-02-11 13:41:30,019 [INFO] Starting deduplication job in batch mode...
2026-02-11 13:41:30,278 [WARNING] CRM raw not Delta, trying Parquet
2026-02-11 13:41:30,318 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 103, in dedupe_source_batch
    df = spark.read.format("parquet").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:41:30,474 [WARNING] ERP raw not Delta, trying Parquet
2026-02-11 13:41:30,536 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 103, in dedupe_source_batch
    df = spark.read.format("parquet").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:41:30,653 [WARNING] Salesforce raw not Delta, trying Parquet
2026-02-11 13:41:30,684 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 103, in dedupe_source_batch
    df = spark.read.format("parquet").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:41:30,795 [WARNING] SAP raw not Delta, trying Parquet
2026-02-11 13:41:30,845 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 103, in dedupe_source_batch
    df = spark.read.format("parquet").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:41:30,971 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 184, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:41:31,871 [INFO] Closing down clientserver connection
2026-02-11 13:52:39,276 [INFO] Starting deduplication job in batch mode...
2026-02-11 13:52:39,466 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer not readable as Delta or Parquet, skipping.
2026-02-11 13:52:39,513 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:52:39,669 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer not readable as Delta or Parquet, skipping.
2026-02-11 13:52:39,731 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:52:39,881 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer not readable as Delta or Parquet, skipping.
2026-02-11 13:52:39,922 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:52:40,081 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer not readable as Delta or Parquet, skipping.
2026-02-11 13:52:40,127 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:52:40,252 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 203, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:52:40,533 [INFO] Closing down clientserver connection
2026-02-11 13:57:31,648 [INFO] Starting deduplication job in batch mode...
2026-02-11 13:57:31,822 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer not readable as Delta or Parquet, skipping.
2026-02-11 13:57:31,885 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:57:32,025 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer not readable as Delta or Parquet, skipping.
2026-02-11 13:57:32,088 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:57:32,229 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer not readable as Delta or Parquet, skipping.
2026-02-11 13:57:32,275 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:57:32,449 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer not readable as Delta or Parquet, skipping.
2026-02-11 13:57:32,495 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:57:32,856 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 210, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 13:57:33,060 [INFO] Closing down clientserver connection
2026-02-11 14:03:07,621 [INFO] Starting deduplication job in batch mode...
2026-02-11 14:03:07,811 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer not readable as Delta or Parquet, skipping.
2026-02-11 14:03:07,857 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 14:03:08,029 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer not readable as Delta or Parquet, skipping.
2026-02-11 14:03:08,076 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 14:03:08,236 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer not readable as Delta or Parquet, skipping.
2026-02-11 14:03:08,273 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 14:03:08,436 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer not readable as Delta or Parquet, skipping.
2026-02-11 14:03:08,483 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 14:03:08,592 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 210, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 14:03:08,865 [INFO] Closing down clientserver connection
2026-02-11 14:13:48,026 [INFO] Starting deduplication job in batch mode...
2026-02-11 14:13:48,214 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer not readable as Delta or Parquet, skipping.
2026-02-11 14:13:48,261 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 14:13:48,432 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer not readable as Delta or Parquet, skipping.
2026-02-11 14:13:48,479 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 14:13:48,636 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer not readable as Delta or Parquet, skipping.
2026-02-11 14:13:48,682 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 14:13:48,857 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer not readable as Delta or Parquet, skipping.
2026-02-11 14:13:48,888 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 14:13:48,997 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 210, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 14:13:49,218 [INFO] Closing down clientserver connection
2026-02-11 16:46:34,534 [INFO] Starting deduplication job in batch mode...
2026-02-11 16:46:34,722 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer not readable as Delta or Parquet, skipping.
2026-02-11 16:46:34,768 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 16:46:34,926 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer not readable as Delta or Parquet, skipping.
2026-02-11 16:46:34,973 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 16:46:35,129 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer not readable as Delta or Parquet, skipping.
2026-02-11 16:46:35,176 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 16:46:35,332 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer not readable as Delta or Parquet, skipping.
2026-02-11 16:46:35,381 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 122, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 16:46:35,506 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 216, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 16:46:35,741 [INFO] Closing down clientserver connection
2026-02-11 17:01:22,453 [INFO] Starting deduplication job in batch mode...
2026-02-11 17:01:22,625 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:01:22,767 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:01:22,908 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:01:23,001 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:01:23,158 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:01:23,268 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:01:23,411 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:01:23,520 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:01:23,630 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 225, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:01:24,195 [INFO] Closing down clientserver connection
2026-02-11 17:06:21,060 [INFO] Starting deduplication job in batch mode...
2026-02-11 17:06:21,232 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:06:21,342 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:06:21,485 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:06:21,578 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:06:21,735 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:06:21,844 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:06:21,997 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:06:22,082 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:06:22,191 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 225, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:06:22,819 [INFO] Closing down clientserver connection
2026-02-11 17:07:07,057 [INFO] Starting deduplication job in batch mode...
2026-02-11 17:07:07,229 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:07:07,351 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:07:07,497 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:07:07,591 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:07:07,747 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:07:07,857 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:07:07,999 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:07:08,109 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 128, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:07:08,218 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 225, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:07:08,800 [INFO] Closing down clientserver connection
2026-02-11 17:16:34,602 [INFO] Starting deduplication job in batch mode...
2026-02-11 17:16:34,602 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 17:16:34,774 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:16:34,883 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 129, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:16:34,931 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 17:16:35,026 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:16:35,120 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 129, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:16:35,183 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 17:16:35,261 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:16:35,354 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 129, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:16:35,417 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 17:16:35,497 [WARNING] C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer not readable as Delta or Parquet, skipping.
2026-02-11 17:16:35,591 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 129, in dedupe_source_batch
    logger.warning(f"Delta read failed for {src['name']} at {raw_path}, trying Parquet. Error: {e}")
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 73, in __str__
    sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:16:35,700 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 226, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:16:35,826 [INFO] Closing down clientserver connection
2026-02-11 17:21:33,054 [INFO] Starting deduplication job in batch mode...
2026-02-11 17:21:33,054 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 17:21:33,163 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 124, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:21:33,241 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 17:21:33,288 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 124, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:21:33,351 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 17:21:33,399 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 124, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:21:33,446 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 17:21:33,493 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 124, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:21:33,618 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 218, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:21:33,794 [INFO] Closing down clientserver connection
2026-02-11 17:34:32,750 [INFO] Starting deduplication job in batch mode...
2026-02-11 17:34:32,750 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 17:34:32,859 [ERROR] Batch deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 124, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:34:32,953 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 17:34:33,000 [ERROR] Batch deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 124, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:34:33,063 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 17:34:33,110 [ERROR] Batch deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 124, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:34:33,156 [INFO] Looking for raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 17:34:33,220 [ERROR] Batch deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 124, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:34:33,331 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 218, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:34:33,487 [INFO] Closing down clientserver connection
2026-02-11 17:38:15,213 [INFO] Starting deduplication job in batch mode...
2026-02-11 17:38:15,213 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 17:38:15,341 [ERROR] Source CRM failed during deduplication
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 193, in main
    clean, conflicted = dedupe_source_batch(src, run_id)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:38:15,418 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 17:38:15,449 [ERROR] Source ERP failed during deduplication
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 193, in main
    clean, conflicted = dedupe_source_batch(src, run_id)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:38:15,511 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 17:38:15,558 [ERROR] Source Salesforce failed during deduplication
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 193, in main
    clean, conflicted = dedupe_source_batch(src, run_id)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:38:15,611 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 17:38:15,664 [ERROR] Source SAP failed during deduplication
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 193, in main
    clean, conflicted = dedupe_source_batch(src, run_id)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1515, in read
    return DataFrameReader(self)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 69, in __init__
    self._jreader = spark._jsparkSession.read()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:38:15,786 [ERROR] Deduplication job failed
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 212, in main
    audit_df = spark.createDataFrame(audit_data, audit_columns)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1276, in createDataFrame
    return self._create_dataframe(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 1318, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 962, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 833, in _inferSchemaFromList
    infer_dict_as_struct = self._jconf.inferDictAsStruct()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\session.py", line 560, in _jconf
    return self._jsparkSession.sessionState().conf()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: <unprintable IllegalArgumentException object>
2026-02-11 17:38:15,942 [INFO] Closing down clientserver connection
2026-02-11 17:53:20,502 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 17:53:44,832 [INFO] Initial row count for CRM: 16
2026-02-11 17:53:45,084 [INFO] Writing Bronze tables for CRM...
2026-02-11 17:54:11,908 [INFO] CRM deduplication complete. Clean=5, Conflicted=11
2026-02-11 17:54:11,908 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 17:54:16,026 [INFO] Initial row count for ERP: 17
2026-02-11 17:54:16,104 [INFO] Writing Bronze tables for ERP...
2026-02-11 17:54:29,996 [INFO] ERP deduplication complete. Clean=17, Conflicted=0
2026-02-11 17:54:29,996 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 17:54:33,212 [INFO] Initial row count for Salesforce: 17
2026-02-11 17:54:33,315 [INFO] Writing Bronze tables for Salesforce...
2026-02-11 17:54:46,464 [INFO] Salesforce deduplication complete. Clean=10, Conflicted=7
2026-02-11 17:54:46,464 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 17:54:49,095 [INFO] Initial row count for SAP: 17
2026-02-11 17:54:49,164 [INFO] Writing Bronze tables for SAP...
2026-02-11 17:55:00,679 [INFO] SAP deduplication complete. Clean=17, Conflicted=0
2026-02-11 17:55:00,679 [INFO] Deduplication complete. Run ID=3224fda4-0a9d-44db-b97c-95cb239f12f2, Clean=49, Conflicted=18, Duration=100.18s
2026-02-11 17:55:01,715 [INFO] Closing down clientserver connection
2026-02-11 18:12:12,075 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 18:12:34,614 [INFO] Initial row count for CRM: 16
2026-02-11 18:12:34,899 [INFO] Writing Bronze tables for CRM...
2026-02-11 18:13:02,273 [INFO] CRM deduplication complete. Clean=5, Conflicted=11
2026-02-11 18:13:02,273 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 18:13:05,201 [INFO] Initial row count for ERP: 17
2026-02-11 18:13:05,315 [INFO] Writing Bronze tables for ERP...
2026-02-11 18:13:25,490 [INFO] ERP deduplication complete. Clean=17, Conflicted=0
2026-02-11 18:13:25,490 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 18:13:28,143 [INFO] Initial row count for Salesforce: 17
2026-02-11 18:13:28,227 [INFO] Writing Bronze tables for Salesforce...
2026-02-11 18:13:45,270 [INFO] Salesforce deduplication complete. Clean=10, Conflicted=7
2026-02-11 18:13:45,270 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 18:13:48,269 [INFO] Initial row count for SAP: 17
2026-02-11 18:13:48,344 [INFO] Writing Bronze tables for SAP...
2026-02-11 18:14:02,402 [INFO] SAP deduplication complete. Clean=17, Conflicted=0
2026-02-11 18:14:10,643 [INFO] Deduplication complete. Run ID=dcdca371-d504-42c0-8a0e-391c340947ec, Clean=49, Conflicted=18, Duration=110.33s
2026-02-11 18:14:13,683 [INFO] Closing down clientserver connection
2026-02-11 18:27:44,912 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 18:28:08,951 [INFO] Initial row count for CRM: 16
2026-02-11 18:28:09,310 [INFO] Writing Bronze tables for CRM with checkpoints...
2026-02-11 18:28:34,355 [INFO] CRM deduplication complete. Clean=5, Conflicted=11
2026-02-11 18:28:34,355 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 18:28:38,538 [INFO] Initial row count for ERP: 17
2026-02-11 18:28:38,640 [INFO] Writing Bronze tables for ERP with checkpoints...
2026-02-11 18:28:57,061 [INFO] ERP deduplication complete. Clean=17, Conflicted=0
2026-02-11 18:28:57,061 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 18:29:00,669 [INFO] Initial row count for Salesforce: 17
2026-02-11 18:29:00,781 [INFO] Writing Bronze tables for Salesforce with checkpoints...
2026-02-11 18:29:14,529 [INFO] Salesforce deduplication complete. Clean=10, Conflicted=7
2026-02-11 18:29:14,529 [INFO] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 18:29:17,956 [INFO] Initial row count for SAP: 17
2026-02-11 18:29:18,022 [INFO] Writing Bronze tables for SAP with checkpoints...
2026-02-11 18:29:29,887 [INFO] SAP deduplication complete. Clean=17, Conflicted=0
2026-02-11 18:29:40,029 [INFO] Deduplication complete. Run ID=0d973267-0349-41b2-87b8-c80baf5b3749, Clean=49, Conflicted=18, Duration=104.97s
2026-02-11 18:29:43,455 [INFO] Closing down clientserver connection
2026-02-11 18:45:27,138 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 18:45:46,791 [INFO] [BATCH] Initial row count for CRM: 16
2026-02-11 18:46:06,984 [INFO] [BATCH] CRM deduplication complete. Clean=5, Conflicted=11
2026-02-11 18:46:06,984 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 18:46:10,246 [INFO] [BATCH] Initial row count for ERP: 17
2026-02-11 18:46:24,850 [INFO] [BATCH] ERP deduplication complete. Clean=17, Conflicted=0
2026-02-11 18:46:24,850 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 18:46:27,439 [INFO] [BATCH] Initial row count for Salesforce: 17
2026-02-11 18:46:38,671 [INFO] [BATCH] Salesforce deduplication complete. Clean=10, Conflicted=7
2026-02-11 18:46:38,687 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 18:46:40,913 [INFO] [BATCH] Initial row count for SAP: 17
2026-02-11 18:46:50,782 [INFO] [BATCH] SAP deduplication complete. Clean=17, Conflicted=0
2026-02-11 18:47:00,635 [INFO] Deduplication complete. Run ID=c2f9f6d2-1e20-44cd-a84c-50aed21e6d39, Clean=49, Conflicted=18, Duration=83.64s
2026-02-11 18:47:02,115 [INFO] Closing down clientserver connection
2026-02-11 18:50:02,317 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 18:50:12,028 [ERROR] [STREAMING] Deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(email#48, phone#49, ingestion_ts#55 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#70], [email#48, phone#49], [ingestion_ts#55 DESC NULLS LAST]
+- Project [customer_id#45, first_name#46, last_name#47, email#48, phone#49, address#50, city#51, country#52, created_ts#53, source_system#54, ingestion_ts#55, run_id#56]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4e3e5b52,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer),None), delta, [customer_id#45, first_name#46, last_name#47, email#48, phone#49, address#50, city#51, country#52, created_ts#53, source_system#54, ingestion_ts#55, run_id#56]

2026-02-11 18:50:12,156 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 18:50:12,852 [ERROR] [STREAMING] Deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(customer_id#193, ingestion_ts#203 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#218], [customer_id#193], [ingestion_ts#203 DESC NULLS LAST]
+- Project [customer_id#193, first_name#194, last_name#195, email#196, phone#197, address#198, city#199, country#200, created_ts#201, source_system#202, ingestion_ts#203, run_id#204]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4e3e5b52,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer),None), delta, [customer_id#193, first_name#194, last_name#195, email#196, phone#197, address#198, city#199, country#200, created_ts#201, source_system#202, ingestion_ts#203, run_id#204]

2026-02-11 18:50:12,859 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 18:50:13,500 [ERROR] [STREAMING] Deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(email#344, ingestion_ts#351 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#366], [email#344], [ingestion_ts#351 DESC NULLS LAST]
+- Project [customer_id#341, first_name#342, last_name#343, email#344, phone#345, address#346, city#347, country#348, created_ts#349, source_system#350, ingestion_ts#351, run_id#352]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4e3e5b52,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer),None), delta, [customer_id#341, first_name#342, last_name#343, email#344, phone#345, address#346, city#347, country#348, created_ts#349, source_system#350, ingestion_ts#351, run_id#352]

2026-02-11 18:50:13,500 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 18:50:14,064 [ERROR] [STREAMING] Deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(customer_id#489, ingestion_ts#499 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#514], [customer_id#489], [ingestion_ts#499 DESC NULLS LAST]
+- Project [customer_id#489, first_name#490, last_name#491, email#492, phone#493, address#494, city#495, country#496, created_ts#497, source_system#498, ingestion_ts#499, run_id#500]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4e3e5b52,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer),None), delta, [customer_id#489, first_name#490, last_name#491, email#492, phone#493, address#494, city#495, country#496, created_ts#497, source_system#498, ingestion_ts#499, run_id#500]

2026-02-11 18:50:14,064 [INFO] Streaming deduplication running... awaiting termination.
2026-02-11 19:10:40,809 [INFO] Error while sending or receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
2026-02-11 19:10:40,887 [INFO] Closing down clientserver connection
2026-02-11 19:10:40,888 [INFO] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2026-02-11 19:10:42,912 [INFO] Closing down clientserver connection
2026-02-11 19:10:42,912 [INFO] Error while receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2026-02-11 19:10:42,912 [INFO] Closing down clientserver connection
2026-02-11 19:10:42,912 [ERROR] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-02-11 19:10:42,912 [INFO] Closing down clientserver connection
2026-02-11 19:10:42,912 [INFO] Closing down clientserver connection
2026-02-11 19:10:42,960 [INFO] Closing down clientserver connection
2026-02-11 19:11:52,999 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 19:12:07,521 [ERROR] [STREAMING] Deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(email#48, phone#49, ingestion_ts#55 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#70], [email#48, phone#49], [ingestion_ts#55 DESC NULLS LAST]
+- Project [customer_id#45, first_name#46, last_name#47, email#48, phone#49, address#50, city#51, country#52, created_ts#53, source_system#54, ingestion_ts#55, run_id#56]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4eca8d59,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer),None), delta, [customer_id#45, first_name#46, last_name#47, email#48, phone#49, address#50, city#51, country#52, created_ts#53, source_system#54, ingestion_ts#55, run_id#56]

2026-02-11 19:12:07,547 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 19:12:08,174 [ERROR] [STREAMING] Deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(customer_id#193, ingestion_ts#203 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#218], [customer_id#193], [ingestion_ts#203 DESC NULLS LAST]
+- Project [customer_id#193, first_name#194, last_name#195, email#196, phone#197, address#198, city#199, country#200, created_ts#201, source_system#202, ingestion_ts#203, run_id#204]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4eca8d59,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer),None), delta, [customer_id#193, first_name#194, last_name#195, email#196, phone#197, address#198, city#199, country#200, created_ts#201, source_system#202, ingestion_ts#203, run_id#204]

2026-02-11 19:12:08,180 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 19:12:09,092 [ERROR] [STREAMING] Deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(email#344, ingestion_ts#351 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#366], [email#344], [ingestion_ts#351 DESC NULLS LAST]
+- Project [customer_id#341, first_name#342, last_name#343, email#344, phone#345, address#346, city#347, country#348, created_ts#349, source_system#350, ingestion_ts#351, run_id#352]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4eca8d59,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer),None), delta, [customer_id#341, first_name#342, last_name#343, email#344, phone#345, address#346, city#347, country#348, created_ts#349, source_system#350, ingestion_ts#351, run_id#352]

2026-02-11 19:12:09,092 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 19:12:10,194 [ERROR] [STREAMING] Deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(customer_id#489, ingestion_ts#499 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#514], [customer_id#489], [ingestion_ts#499 DESC NULLS LAST]
+- Project [customer_id#489, first_name#490, last_name#491, email#492, phone#493, address#494, city#495, country#496, created_ts#497, source_system#498, ingestion_ts#499, run_id#500]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4eca8d59,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer),None), delta, [customer_id#489, first_name#490, last_name#491, email#492, phone#493, address#494, city#495, country#496, created_ts#497, source_system#498, ingestion_ts#499, run_id#500]

2026-02-11 19:12:10,208 [INFO] Streaming deduplication running... awaiting termination.
2026-02-11 19:15:11,322 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-11 19:15:23,363 [ERROR] [STREAMING] Deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(email#48, phone#49, ingestion_ts#55 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#70], [email#48, phone#49], [ingestion_ts#55 DESC NULLS LAST]
+- Project [customer_id#45, first_name#46, last_name#47, email#48, phone#49, address#50, city#51, country#52, created_ts#53, source_system#54, ingestion_ts#55, run_id#56]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@18c24d6,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer),None), delta, [customer_id#45, first_name#46, last_name#47, email#48, phone#49, address#50, city#51, country#52, created_ts#53, source_system#54, ingestion_ts#55, run_id#56]

2026-02-11 19:15:23,393 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-11 19:15:24,347 [ERROR] [STREAMING] Deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(customer_id#193, ingestion_ts#203 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#218], [customer_id#193], [ingestion_ts#203 DESC NULLS LAST]
+- Project [customer_id#193, first_name#194, last_name#195, email#196, phone#197, address#198, city#199, country#200, created_ts#201, source_system#202, ingestion_ts#203, run_id#204]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@18c24d6,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer),None), delta, [customer_id#193, first_name#194, last_name#195, email#196, phone#197, address#198, city#199, country#200, created_ts#201, source_system#202, ingestion_ts#203, run_id#204]

2026-02-11 19:15:24,347 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-11 19:15:25,167 [ERROR] [STREAMING] Deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(email#344, ingestion_ts#351 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#366], [email#344], [ingestion_ts#351 DESC NULLS LAST]
+- Project [customer_id#341, first_name#342, last_name#343, email#344, phone#345, address#346, city#347, country#348, created_ts#349, source_system#350, ingestion_ts#351, run_id#352]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@18c24d6,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer),None), delta, [customer_id#341, first_name#342, last_name#343, email#344, phone#345, address#346, city#347, country#348, created_ts#349, source_system#350, ingestion_ts#351, run_id#352]

2026-02-11 19:15:25,167 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-11 19:15:25,915 [ERROR] [STREAMING] Deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(customer_id#489, ingestion_ts#499 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#514], [customer_id#489], [ingestion_ts#499 DESC NULLS LAST]
+- Project [customer_id#489, first_name#490, last_name#491, email#492, phone#493, address#494, city#495, country#496, created_ts#497, source_system#498, ingestion_ts#499, run_id#500]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@18c24d6,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer),None), delta, [customer_id#489, first_name#490, last_name#491, email#492, phone#493, address#494, city#495, country#496, created_ts#497, source_system#498, ingestion_ts#499, run_id#500]

2026-02-11 19:15:25,915 [INFO] Streaming deduplication running... awaiting termination.
2026-02-12 09:53:56,298 [INFO] Error while sending or receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
2026-02-12 09:53:56,575 [INFO] Closing down clientserver connection
2026-02-12 09:53:56,610 [INFO] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2026-02-12 09:53:58,670 [INFO] Closing down clientserver connection
2026-02-12 09:53:58,672 [INFO] Error while receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2026-02-12 09:53:58,699 [INFO] Closing down clientserver connection
2026-02-12 09:53:58,700 [ERROR] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-02-12 09:53:58,703 [INFO] Closing down clientserver connection
2026-02-12 09:53:58,703 [INFO] Closing down clientserver connection
2026-02-12 09:53:59,104 [INFO] Closing down clientserver connection
2026-02-12 09:54:46,184 [INFO] Error while sending or receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
2026-02-12 09:54:46,312 [INFO] Closing down clientserver connection
2026-02-12 09:54:46,318 [INFO] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2026-02-12 09:54:48,345 [INFO] Closing down clientserver connection
2026-02-12 09:54:48,346 [INFO] Error while receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2026-02-12 09:54:48,497 [INFO] Closing down clientserver connection
2026-02-12 09:54:48,499 [ERROR] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-02-12 09:54:48,500 [INFO] Closing down clientserver connection
2026-02-12 09:54:48,500 [INFO] Closing down clientserver connection
2026-02-12 09:54:48,880 [INFO] Closing down clientserver connection
2026-02-12 09:57:16,304 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 09:57:59,381 [ERROR] [STREAMING] Deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(email#48, phone#49, ingestion_ts#55 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#70], [email#48, phone#49], [ingestion_ts#55 DESC NULLS LAST]
+- Project [customer_id#45, first_name#46, last_name#47, email#48, phone#49, address#50, city#51, country#52, created_ts#53, source_system#54, ingestion_ts#55, run_id#56]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@9278d35,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer),None), delta, [customer_id#45, first_name#46, last_name#47, email#48, phone#49, address#50, city#51, country#52, created_ts#53, source_system#54, ingestion_ts#55, run_id#56]

2026-02-12 09:57:59,406 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 09:58:01,547 [ERROR] [STREAMING] Deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(customer_id#193, ingestion_ts#203 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#218], [customer_id#193], [ingestion_ts#203 DESC NULLS LAST]
+- Project [customer_id#193, first_name#194, last_name#195, email#196, phone#197, address#198, city#199, country#200, created_ts#201, source_system#202, ingestion_ts#203, run_id#204]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@9278d35,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer),None), delta, [customer_id#193, first_name#194, last_name#195, email#196, phone#197, address#198, city#199, country#200, created_ts#201, source_system#202, ingestion_ts#203, run_id#204]

2026-02-12 09:58:01,557 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 09:58:06,582 [ERROR] [STREAMING] Deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(email#344, ingestion_ts#351 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#366], [email#344], [ingestion_ts#351 DESC NULLS LAST]
+- Project [customer_id#341, first_name#342, last_name#343, email#344, phone#345, address#346, city#347, country#348, created_ts#349, source_system#350, ingestion_ts#351, run_id#352]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@9278d35,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer),None), delta, [customer_id#341, first_name#342, last_name#343, email#344, phone#345, address#346, city#347, country#348, created_ts#349, source_system#350, ingestion_ts#351, run_id#352]

2026-02-12 09:58:06,597 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 09:58:09,282 [ERROR] [STREAMING] Deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(customer_id#489, ingestion_ts#499 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#514], [customer_id#489], [ingestion_ts#499 DESC NULLS LAST]
+- Project [customer_id#489, first_name#490, last_name#491, email#492, phone#493, address#494, city#495, country#496, created_ts#497, source_system#498, ingestion_ts#499, run_id#500]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@9278d35,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer),None), delta, [customer_id#489, first_name#490, last_name#491, email#492, phone#493, address#494, city#495, country#496, created_ts#497, source_system#498, ingestion_ts#499, run_id#500]

2026-02-12 09:58:09,298 [INFO] Streaming deduplication running... awaiting termination.
2026-02-12 10:36:18,632 [INFO] Error while sending or receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
2026-02-12 10:36:18,749 [INFO] Closing down clientserver connection
2026-02-12 10:36:18,751 [INFO] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2026-02-12 10:36:20,788 [INFO] Closing down clientserver connection
2026-02-12 10:36:20,790 [INFO] Error while receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2026-02-12 10:36:20,791 [INFO] Closing down clientserver connection
2026-02-12 10:36:20,791 [ERROR] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-02-12 10:36:20,792 [INFO] Closing down clientserver connection
2026-02-12 10:36:20,792 [INFO] Closing down clientserver connection
2026-02-12 10:36:20,902 [INFO] Closing down clientserver connection
2026-02-12 10:37:11,912 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 10:37:27,348 [ERROR] [STREAMING] Deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(email#48, phone#49, ingestion_ts#55 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#70], [email#48, phone#49], [ingestion_ts#55 DESC NULLS LAST]
+- Project [customer_id#45, first_name#46, last_name#47, email#48, phone#49, address#50, city#51, country#52, created_ts#53, source_system#54, ingestion_ts#55, run_id#56]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7b2757b6,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer),None), delta, [customer_id#45, first_name#46, last_name#47, email#48, phone#49, address#50, city#51, country#52, created_ts#53, source_system#54, ingestion_ts#55, run_id#56]

2026-02-12 10:37:27,362 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 10:37:28,052 [ERROR] [STREAMING] Deduplication failed for ERP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(customer_id#193, ingestion_ts#203 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#218], [customer_id#193], [ingestion_ts#203 DESC NULLS LAST]
+- Project [customer_id#193, first_name#194, last_name#195, email#196, phone#197, address#198, city#199, country#200, created_ts#201, source_system#202, ingestion_ts#203, run_id#204]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7b2757b6,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer),None), delta, [customer_id#193, first_name#194, last_name#195, email#196, phone#197, address#198, city#199, country#200, created_ts#201, source_system#202, ingestion_ts#203, run_id#204]

2026-02-12 10:37:28,056 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 10:37:28,831 [ERROR] [STREAMING] Deduplication failed for Salesforce
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(email#344, ingestion_ts#351 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#366], [email#344], [ingestion_ts#351 DESC NULLS LAST]
+- Project [customer_id#341, first_name#342, last_name#343, email#344, phone#345, address#346, city#347, country#348, created_ts#349, source_system#350, ingestion_ts#351, run_id#352]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7b2757b6,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer),None), delta, [customer_id#341, first_name#342, last_name#343, email#344, phone#345, address#346, city#347, country#348, created_ts#349, source_system#350, ingestion_ts#351, run_id#352]

2026-02-12 10:37:28,838 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 10:37:29,881 [ERROR] [STREAMING] Deduplication failed for SAP
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 123, in dedupe_source_streaming
    .start(str(main_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\streaming\readwriter.py", line 1387, in start
    return self._sq(self._jwrite.start(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;
Window [row_number() windowspecdefinition(customer_id#489, ingestion_ts#499 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#514], [customer_id#489], [ingestion_ts#499 DESC NULLS LAST]
+- Project [customer_id#489, first_name#490, last_name#491, email#492, phone#493, address#494, city#495, country#496, created_ts#497, source_system#498, ingestion_ts#499, run_id#500]
   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7b2757b6,delta,List(),None,List(),None,Map(path -> C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer),None), delta, [customer_id#489, first_name#490, last_name#491, email#492, phone#493, address#494, city#495, country#496, created_ts#497, source_system#498, ingestion_ts#499, run_id#500]

2026-02-12 10:37:29,883 [INFO] Streaming deduplication running... awaiting termination.
2026-02-12 11:31:41,697 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 11:32:02,865 [INFO] [BATCH] Initial row count for CRM: 66
2026-02-12 11:32:28,999 [INFO] [BATCH] CRM deduplication complete. Clean=5, Conflicted=61
2026-02-12 11:32:28,999 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 11:32:32,656 [INFO] [BATCH] Initial row count for ERP: 63
2026-02-12 11:32:49,310 [INFO] [BATCH] ERP deduplication complete. Clean=23, Conflicted=40
2026-02-12 11:32:49,310 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 11:32:52,615 [INFO] [BATCH] Initial row count for Salesforce: 63
2026-02-12 11:33:04,762 [INFO] [BATCH] Salesforce deduplication complete. Clean=12, Conflicted=51
2026-02-12 11:33:04,762 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 11:33:09,037 [INFO] [BATCH] Initial row count for SAP: 63
2026-02-12 11:33:24,134 [INFO] [BATCH] SAP deduplication complete. Clean=23, Conflicted=40
2026-02-12 11:33:35,969 [INFO] Deduplication complete. Run ID=767ad9bc-d374-4c73-b85c-2434a484e512, Clean=63, Conflicted=192, Duration=102.44s
2026-02-12 11:33:39,248 [INFO] Closing down clientserver connection
2026-02-12 11:38:13,386 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 11:38:25,437 [INFO] [STREAMING] CRM streaming deduplication started.
2026-02-12 11:38:25,437 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 11:38:27,943 [INFO] [STREAMING] ERP streaming deduplication started.
2026-02-12 11:38:27,943 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 11:38:31,625 [INFO] [STREAMING] Salesforce streaming deduplication started.
2026-02-12 11:38:31,626 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 11:38:44,841 [INFO] [STREAMING] SAP streaming deduplication started.
2026-02-12 11:38:44,841 [INFO] Streaming deduplication running... awaiting termination.
2026-02-12 11:38:58,013 [INFO] Closing down clientserver connection
2026-02-12 12:11:15,069 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 12:11:35,329 [INFO] [BATCH] Initial row count for CRM: 98
2026-02-12 12:11:55,833 [INFO] [BATCH] CRM deduplication complete. Clean=7, Conflicted=91
2026-02-12 12:11:55,833 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 12:11:59,128 [INFO] [BATCH] Initial row count for ERP: 91
2026-02-12 12:12:15,020 [INFO] [BATCH] ERP deduplication complete. Clean=45, Conflicted=46
2026-02-12 12:12:15,020 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 12:12:17,839 [INFO] [BATCH] Initial row count for Salesforce: 91
2026-02-12 12:12:31,036 [INFO] [BATCH] Salesforce deduplication complete. Clean=17, Conflicted=74
2026-02-12 12:12:31,036 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 12:12:34,264 [INFO] [BATCH] Initial row count for SAP: 91
2026-02-12 12:12:45,249 [INFO] [BATCH] SAP deduplication complete. Clean=45, Conflicted=46
2026-02-12 12:12:56,371 [INFO] Deduplication complete. Run ID=bd69088c-0199-4319-83f3-2fb5d7c0a6d5, Clean=114, Conflicted=257, Duration=90.18s
2026-02-12 12:12:59,256 [INFO] Closing down clientserver connection
2026-02-12 12:15:12,173 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 12:15:24,985 [INFO] [STREAMING] CRM streaming deduplication started.
2026-02-12 12:15:24,985 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 12:15:25,845 [INFO] [STREAMING] ERP streaming deduplication started.
2026-02-12 12:15:25,845 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 12:15:26,879 [INFO] [STREAMING] Salesforce streaming deduplication started.
2026-02-12 12:15:26,879 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 12:15:28,668 [INFO] [STREAMING] SAP streaming deduplication started.
2026-02-12 12:15:28,668 [INFO] Streaming deduplication running... awaiting termination.
2026-02-12 12:15:56,343 [INFO] Closing down clientserver connection
2026-02-12 12:22:24,710 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 12:22:37,337 [INFO] [STREAMING] CRM streaming deduplication started.
2026-02-12 12:22:37,337 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 12:22:38,174 [INFO] [STREAMING] ERP streaming deduplication started.
2026-02-12 12:22:38,174 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 12:22:39,540 [INFO] [STREAMING] Salesforce streaming deduplication started.
2026-02-12 12:22:39,634 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 12:22:40,637 [INFO] [STREAMING] SAP streaming deduplication started.
2026-02-12 12:22:40,637 [INFO] Streaming deduplication running... awaiting termination.
2026-02-12 12:23:10,965 [INFO] Closing down clientserver connection
2026-02-12 12:29:32,074 [INFO] Error while sending or receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
2026-02-12 12:29:32,298 [INFO] Closing down clientserver connection
2026-02-12 12:29:32,314 [INFO] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2026-02-12 12:29:34,381 [INFO] Closing down clientserver connection
2026-02-12 12:29:34,381 [INFO] Error while receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2026-02-12 12:29:34,381 [INFO] Closing down clientserver connection
2026-02-12 12:29:34,381 [ERROR] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-02-12 12:29:34,381 [INFO] Closing down clientserver connection
2026-02-12 12:29:34,381 [INFO] Closing down clientserver connection
2026-02-12 12:29:34,497 [INFO] Closing down clientserver connection
2026-02-12 12:30:56,511 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 12:31:11,196 [INFO] [STREAMING] CRM streaming deduplication started.
2026-02-12 12:31:11,196 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 12:31:12,057 [INFO] [STREAMING] ERP streaming deduplication started.
2026-02-12 12:31:12,057 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 12:31:13,469 [INFO] [STREAMING] Salesforce streaming deduplication started.
2026-02-12 12:31:13,469 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 12:31:15,172 [INFO] [STREAMING] SAP streaming deduplication started.
2026-02-12 12:31:15,172 [INFO] Streaming deduplication running... awaiting termination.
2026-02-12 12:31:43,593 [INFO] Closing down clientserver connection
2026-02-12 13:20:31,608 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 13:20:53,743 [INFO] [STREAMING] CRM streaming deduplication started.
2026-02-12 13:20:53,743 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 13:20:54,957 [INFO] [STREAMING] ERP streaming deduplication started.
2026-02-12 13:20:54,957 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 13:20:57,868 [INFO] [STREAMING] Salesforce streaming deduplication started.
2026-02-12 13:20:57,868 [INFO] [STREAMING] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 13:20:59,605 [INFO] [STREAMING] SAP streaming deduplication started.
2026-02-12 13:20:59,605 [INFO] Streaming deduplication running... awaiting termination.
2026-02-12 14:20:48,169 [INFO] Error while sending or receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
2026-02-12 14:20:49,026 [INFO] Closing down clientserver connection
2026-02-12 14:20:49,088 [INFO] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2026-02-12 14:20:51,192 [INFO] Closing down clientserver connection
2026-02-12 14:20:51,208 [INFO] Error while receiving.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
2026-02-12 14:20:51,208 [INFO] Closing down clientserver connection
2026-02-12 14:20:51,208 [ERROR] Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Users\viddi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-02-12 14:20:51,208 [INFO] Closing down clientserver connection
2026-02-12 14:20:51,208 [INFO] Closing down clientserver connection
2026-02-12 14:20:51,788 [INFO] Closing down clientserver connection
2026-02-12 17:51:24,107 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 17:51:53,878 [INFO] [BATCH] Initial row count for CRM: 168
2026-02-12 17:52:23,452 [INFO] [BATCH] CRM deduplication complete. Clean=7, Conflicted=161
2026-02-12 17:52:37,364 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 17:52:41,500 [INFO] [BATCH] Initial row count for ERP: 153
2026-02-12 17:52:56,783 [INFO] [BATCH] ERP deduplication complete. Clean=49, Conflicted=104
2026-02-12 17:53:04,764 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 17:53:07,918 [INFO] [BATCH] Initial row count for Salesforce: 153
2026-02-12 17:53:21,424 [INFO] [BATCH] Salesforce deduplication complete. Clean=17, Conflicted=136
2026-02-12 17:53:30,090 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 17:53:32,804 [INFO] [BATCH] Initial row count for SAP: 153
2026-02-12 17:53:46,811 [INFO] [BATCH] SAP deduplication complete. Clean=49, Conflicted=104
2026-02-12 17:54:06,060 [INFO] Deduplication complete. Run ID=ae66385c-f8e0-4310-8034-69e5cd15fc2c, Clean=122, Conflicted=505, Duration=150.72s
2026-02-12 17:54:09,271 [INFO] Closing down clientserver connection
2026-02-12 18:00:38,193 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 18:00:43,093 [ERROR] [BATCH] Deduplication failed for CRM
Traceback (most recent call last):
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\src\deduplication.py", line 77, in dedupe_source_batch
    df = spark.read.format("delta").load(str(raw_path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\sql\readwriter.py", line 300, in load
    return self._df(self._jreader.load(path))
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\viddi\Desktop\CoE\mdm-accelerator\venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: C:/Users/viddi/Desktop/CoE/mdm-accelerator/storage/raw/CRM/customer.
2026-02-12 18:01:11,868 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 18:01:22,380 [INFO] [BATCH] Initial row count for ERP: 153
2026-02-12 18:01:46,508 [INFO] [BATCH] ERP deduplication complete. Clean=49, Conflicted=104
2026-02-12 18:01:56,461 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 18:01:59,741 [INFO] [BATCH] Initial row count for Salesforce: 153
2026-02-12 18:02:11,186 [INFO] [BATCH] Salesforce deduplication complete. Clean=17, Conflicted=136
2026-02-12 18:02:19,162 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 18:02:22,560 [INFO] [BATCH] Initial row count for SAP: 153
2026-02-12 18:02:35,611 [INFO] [BATCH] SAP deduplication complete. Clean=49, Conflicted=104
2026-02-12 18:02:49,770 [WARNING] Deduplication completed with failures. Failed sources: ['CRM']
2026-02-12 18:02:53,573 [INFO] Closing down clientserver connection
2026-02-12 18:05:44,655 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\CRM\customer
2026-02-12 18:06:10,279 [INFO] [BATCH] Initial row count for CRM: 204
2026-02-12 18:06:34,980 [INFO] [BATCH] CRM deduplication complete. Clean=7, Conflicted=197
2026-02-12 18:06:45,737 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\ERP\customer
2026-02-12 18:06:49,442 [INFO] [BATCH] Initial row count for ERP: 185
2026-02-12 18:07:01,779 [INFO] [BATCH] ERP deduplication complete. Clean=49, Conflicted=136
2026-02-12 18:07:08,525 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\Salesforce\customer
2026-02-12 18:07:10,443 [INFO] [BATCH] Initial row count for Salesforce: 185
2026-02-12 18:07:20,976 [INFO] [BATCH] Salesforce deduplication complete. Clean=17, Conflicted=168
2026-02-12 18:07:28,096 [INFO] [BATCH] Reading raw path: C:\Users\viddi\Desktop\CoE\mdm-accelerator\storage\raw\SAP\customer
2026-02-12 18:07:31,613 [INFO] [BATCH] Initial row count for SAP: 185
2026-02-12 18:07:44,365 [INFO] [BATCH] SAP deduplication complete. Clean=49, Conflicted=136
2026-02-12 18:07:59,645 [INFO] Deduplication complete. Run ID=210eadb4-6085-4265-9fa6-b22b0dcf6fde, Clean=122, Conflicted=637, Duration=128.25s
2026-02-12 18:08:02,727 [INFO] Closing down clientserver connection
