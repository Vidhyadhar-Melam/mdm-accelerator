# Block 3 – Silver Layer (Global Dedup + Survivorship + Multi-Blocking Keys + Similarity)

import json, uuid
from datetime import datetime
from pyspark.sql import Window
from pyspark.sql.functions import (
    col, lit, to_date, row_number, when, substring_index, soundex
)
from pyspark.sql.types import (
    StructType, StructField, StringType, TimestampType, LongType, DoubleType
)
from delta.tables import DeltaTable
from difflib import SequenceMatcher

# -------------------------------------------------------------
# Load Configs
# -------------------------------------------------------------
sources_path = "s3://databricks-amz-s3-bucket/mdm-accelerator/config/sources.json"
paths_path   = "s3://databricks-amz-s3-bucket/mdm-accelerator/config/paths.json"
env_path     = "s3://databricks-amz-s3-bucket/mdm-accelerator/config/environment.json"

sources = json.loads(dbutils.fs.head(sources_path, 100000))["sources"]
paths   = json.loads(dbutils.fs.head(paths_path, 100000))
env     = json.loads(dbutils.fs.head(env_path, 100000))

# -------------------------------------------------------------
# Similarity UDF (fuzzy matching)
# -------------------------------------------------------------
def multi_similarity(name1, name2, email1, email2, phone1, phone2, id1, id2):
    score = 0
    if name1 and name2:
        score += 0.3 * SequenceMatcher(None, str(name1), str(name2)).ratio() * 100
    if email1 and email2:
        score += 0.3 * SequenceMatcher(None, str(email1), str(email2)).ratio() * 100
    if phone1 and phone2:
        score += 0.3 * SequenceMatcher(None, str(phone1), str(phone2)).ratio() * 100
    if id1 and id2 and str(id1) == str(id2):
        score += 0.1 * 100
    return score

from pyspark.sql.functions import udf
similarity_udf = udf(multi_similarity, DoubleType())

# -------------------------------------------------------------
# Error Schema
# -------------------------------------------------------------
silver_error_schema = StructType([
    StructField("run_id", StringType(), True),
    StructField("source_system", StringType(), True),
    StructField("error_message", StringType(), True),
    StructField("error_time", TimestampType(), True),
    StructField("environment", StringType(), True)
])

# -------------------------------------------------------------
# Load all Bronze Main tables (union fix)
# -------------------------------------------------------------
def load_all_bronze_main():
    dfs = []
    for src in sources:
        bronze_main_path = f"{paths['bronze_main']}/{src['name'].lower()}"
        df = spark.read.format("delta").load(bronze_main_path)
        dfs.append(df)

    df_all = dfs[0]
    for df in dfs[1:]:
        df_all = df_all.unionByName(df, allowMissingColumns=True)

    return df_all

# -------------------------------------------------------------
# Global Silver Dedup + Survivorship
# -------------------------------------------------------------
def global_silver(run_id, lineage_records):
    try:
        start_time = datetime.now()
        df_all = load_all_bronze_main()
        print("Initial Bronze union count:", df_all.count())

        # ---------------- Data Quality Checks ----------------
        df_all = df_all.filter(col("customer_id").isNotNull())
        df_all = df_all.filter(col("email").isNotNull())
        df_all = df_all.filter(col("phone").isNotNull())
        df_all = df_all.filter(col("email").rlike("^[^@]+@[^@]+\\.[^@]+$"))
        df_all = df_all.filter(col("phone").rlike("^[0-9+\\- ]{7,20}$"))

        # Normalize created_ts
        if "created_ts" in df_all.columns:
            df_all = df_all.withColumn("created_ts", to_date(col("created_ts"), "yyyy-MM-dd"))
        else:
            df_all = df_all.withColumn("created_ts", col("ingestion_ts"))

        print("After quality checks:", df_all.count())

        # ---------------- Multi-Blocking Keys ----------------
        df_all = df_all.withColumn("blocking_email", substring_index(col("email"), "@", -1))
        df_all = df_all.withColumn("blocking_phone", col("phone").substr(1,3))
        df_all = df_all.withColumn("blocking_name", soundex(col("first_name")))

        # ---------------- Survivorship Rules ----------------
        priority_expr = when(col("source_system")=="Salesforce",1) \
                        .when(col("source_system")=="ERP",2) \
                        .when(col("source_system")=="CRM",3) \
                        .when(col("source_system")=="SAP",4) \
                        .otherwise(5)

        w = Window.partitionBy("blocking_email","blocking_phone","blocking_name") \
                  .orderBy(priority_expr.asc(), col("created_ts").desc())
        df_ranked = df_all.withColumn("rank", row_number().over(w))

        df_main = df_ranked.filter(col("rank")==1).drop("rank")
        df_conflicted = df_ranked.filter(col("rank")>1).drop("rank")

        print("Silver Main count (before similarity):", df_main.count())
        print("Silver Conflicted count (before similarity):", df_conflicted.count())

        # ---------------- Similarity Matching ----------------
        joined = df_conflicted.alias("conf").join(
            df_main.alias("main"),
            on=["blocking_email","blocking_phone","blocking_name"], how="inner"
        )
        joined = joined.withColumn("similarity", similarity_udf(
            col("conf.first_name"), col("main.first_name"),
            col("conf.email"), col("main.email"),
            col("conf.phone"), col("main.phone"),
            col("conf.customer_id"), col("main.customer_id")
        ))

        threshold = 80  # lower threshold to catch more duplicates
        df_conflicted = joined.where(col("similarity") >= threshold).select("conf.*")
        print("Silver Conflicted count (after similarity):", df_conflicted.count())

        # ---------------- SCD Type 2 ----------------
        now = datetime.now()
        df_main = (df_main.withColumn("valid_from", lit(now))
                            .withColumn("valid_to", lit(None).cast(TimestampType()))
                            .withColumn("is_current", lit(True)))
        df_conflicted = (df_conflicted.withColumn("valid_from", lit(now))
                                    .withColumn("valid_to", lit(None).cast(TimestampType()))
                                    .withColumn("is_current", lit(True)))

        # ---------------- Write Outputs ----------------
        silver_main_path = f"{paths['silver_main']}/global"
        silver_conflicted_path = f"{paths['silver_conflicted']}/global"

        df_main.write.format("delta").mode("overwrite").option("mergeSchema","true") \
            .partitionBy("source_system").save(silver_main_path)
        df_conflicted.write.format("delta").mode("overwrite").option("mergeSchema","true") \
            .partitionBy("source_system").save(silver_conflicted_path)

        # ---------------- Audit Logging ----------------
        lineage_records.append((run_id,"GLOBAL",paths["bronze_main"],silver_main_path,datetime.now(),
                                df_all.count(),df_main.count(),df_conflicted.count(),env["environment"],"GLOBAL_SURVIVORSHIP"))

        return df_main.count(), df_conflicted.count()

    except Exception as e:
        error_df = spark.createDataFrame(
            [(run_id,"GLOBAL",str(e),datetime.now(),env["environment"])],
            silver_error_schema
        )
        error_df.write.format("delta").mode("append").save(paths["silver_error"])
        print("Error in global_silver:", e)
        return 0,0

# -------------------------------------------------------------
# Entry Point
# -------------------------------------------------------------
def main():
    run_id = str(uuid.uuid4())
    job_name = "Silver-Global"
    job_start = datetime.now()

    lineage_records = []
    clean_count, conflicted_count = global_silver(run_id, lineage_records)

    job_end = datetime.now()
    job_duration = (job_end - job_start).total_seconds()

    print(f"Silver Global complete. Run ID={run_id}, Clean={clean_count}, Conflicted={conflicted_count}, Duration={job_duration}s")

    # ---------------- Audit Runs ----------------
    audit_schema = StructType([
        StructField("run_id", StringType(), True),
        StructField("job_name", StringType(), True),
        StructField("start_time", TimestampType(), True),
        StructField("end_time", TimestampType(), True),
        StructField("duration", DoubleType(), True),
        StructField("records_clean", LongType(), True),
        StructField("records_conflicted", LongType(), True),
        StructField("environment", StringType(), True)
    ])

    audit_data = [(run_id, job_name, job_start, job_end, job_duration,
                   clean_count, conflicted_count, env["environment"])]

    audit_df = spark.createDataFrame(audit_data, schema=audit_schema)
    audit_df.write.format("delta").mode("append").option("mergeSchema","true").save(paths["audit_runs"])

        # ---------------- Audit Lineage ----------------
    lineage_schema = StructType([
        StructField("run_id", StringType(), True),
        StructField("source_system", StringType(), True),
        StructField("source_path", StringType(), True),
        StructField("target_path", StringType(), True),
        StructField("lineage_time", TimestampType(), True),
        StructField("bronze_count", LongType(), True),
        StructField("silver_clean_count", LongType(), True),
        StructField("silver_conflicted_count", LongType(), True),
        StructField("environment", StringType(), True),
        StructField("load_type", StringType(), True)
    ])

    lineage_df = spark.createDataFrame(lineage_records, schema=lineage_schema)
    lineage_df.write.format("delta").mode("append").option("mergeSchema","true").save(paths["audit_lineage"])

    print(f"Silver Global complete. Run ID={run_id}, Clean={clean_count}, Conflicted={conflicted_count}, Duration={job_duration}s")
    # Expect Clean≈25, Conflicted≈95

    # Validation: Show audit tables
    display(spark.read.format("delta").load(paths["audit_runs"]))
    display(spark.read.format("delta").load(paths["audit_lineage"]))

# -------------------------------------------------------------
# Entry Point Trigger
# -------------------------------------------------------------
if __name__ == "__main__":
    main()
