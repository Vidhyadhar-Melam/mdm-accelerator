# Block 3 – Silver Layer (Enhanced Production-Grade)

import json, uuid
from datetime import datetime
from pyspark.sql.functions import col, lit, udf, to_date
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, DoubleType
from delta.tables import DeltaTable
from rapidfuzz import fuzz
from pyspark.sql.utils import AnalysisException

# -------------------------------------------------------------
# Load Configs
# -------------------------------------------------------------
sources_path = "s3://databricks-amz-s3-bucket/mdm-accelerator/config/sources.json"
paths_path   = "s3://databricks-amz-s3-bucket/mdm-accelerator/config/paths.json"
env_path     = "s3://databricks-amz-s3-bucket/mdm-accelerator/config/environment.json"

sources = json.loads(dbutils.fs.head(sources_path, 100000))["sources"]
paths   = json.loads(dbutils.fs.head(paths_path, 100000))
env     = json.loads(dbutils.fs.head(env_path, 100000))

# -------------------------------------------------------------
# Similarity UDF
# -------------------------------------------------------------
def multi_similarity(name1, name2, email1, email2, phone1, phone2, id1, id2):
    score = 0
    if name1 and name2:
        score += 0.3 * fuzz.token_sort_ratio(str(name1), str(name2))
    if email1 and email2:
        score += 0.4 * fuzz.token_sort_ratio(str(email1), str(email2))
    if phone1 and phone2:
        score += 0.2 * fuzz.token_sort_ratio(str(phone1), str(phone2))
    if id1 and id2 and str(id1) == str(id2):
        score += 0.1 * 100
    return score

similarity_udf = udf(multi_similarity, DoubleType())

# -------------------------------------------------------------
# Initialize silver_error upfront (guaranteed folder creation)
# -------------------------------------------------------------
silver_error_schema = StructType([
    StructField("run_id", StringType(), True),
    StructField("source_system", StringType(), True),
    StructField("error_message", StringType(), True),
    StructField("error_time", TimestampType(), True),
    StructField("environment", StringType(), True)
])

# try:
#     spark.read.format("delta").load(paths["silver_error"])
# except AnalysisException:
empty_df = spark.createDataFrame([], silver_error_schema)
empty_df.write.format("delta").mode("overwrite").save(paths["silver_error"])

# -------------------------------------------------------------
# Silver Layer Processing
# -------------------------------------------------------------
def process_source_to_silver(src, run_id, lineage_records, error_records, failed_records):
    try:
        bronze_path = f"s3://databricks-amz-s3-bucket/mdm-accelerator/storage/bronze/main/{src['name'].lower()}"
        df = spark.read.format("delta").load(bronze_path)
        initial_count = df.count()

        if initial_count == 0:
            return 0, 0, 0

        # Data Quality Checks
        df = df.filter(col("customer_id").isNotNull())
        if "email" in df.columns:
            df = df.filter(col("email").rlike("^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$"))
        if "phone" in df.columns:
            df = df.filter(col("phone").rlike("^[0-9]{10,}$"))

        # Remove duplicates
        df = df.dropDuplicates(["customer_id", "email", "phone"])

        # Incremental Load (first run = full load)
        last_run_df = spark.read.format("delta").load(paths["audit_runs"]).filter(col("job_name") == "Silver-Layer")
        if last_run_df.count() > 0:
            last_run_time = last_run_df.agg({"end_time":"max"}).collect()[0][0]
            df = df.filter(col("ingestion_ts") > lit(last_run_time))
            load_type = "INCREMENTAL"
        else:
            load_type = "FULL"  # first run → load everything

        # Add ingestion_date
        if "ingestion_ts" in df.columns:
            df = df.withColumn("ingestion_date", to_date(col("ingestion_ts")))

        # Silver Paths
        silver_main_path = f"s3://databricks-amz-s3-bucket/mdm-accelerator/storage/silver/main/{src['name'].lower()}"
        silver_conflicted_path = f"s3://databricks-amz-s3-bucket/mdm-accelerator/storage/silver/conflicted/{src['name'].lower()}"

        compared_count = 0

        if DeltaTable.isDeltaTable(spark, silver_main_path):
            silver_df = spark.read.format("delta").load(silver_main_path)

            joined = df.alias("new").crossJoin(silver_df.alias("existing"))

            joined = joined.withColumn("similarity", similarity_udf(
                col("new.first_name"), col("existing.first_name"),
                col("new.email"), col("existing.email"),
                col("new.phone"), col("existing.phone"),
                col("new.customer_id"), col("existing.customer_id")
            ))

            threshold = src.get("similarity_threshold", 80)

            conflicted = joined.filter(col("similarity") >= threshold).select("new.*")
            new_records = joined.filter(col("similarity") < threshold).select("new.*")

            compared_count = joined.count()

        else:
            # First run → all records are new
            new_records = df
            conflicted = spark.createDataFrame([], df.schema)

        # Add SCD Type 2 fields
        now = datetime.now()
        new_records = (new_records
                       .withColumn("valid_from", lit(now))
                       .withColumn("valid_to", lit(None).cast(TimestampType()))
                       .withColumn("is_current", lit(True)))

        conflicted = (conflicted
                      .withColumn("valid_from", lit(now))
                      .withColumn("valid_to", lit(None).cast(TimestampType()))
                      .withColumn("is_current", lit(True)))

        # Write outputs (batch mode, no checkpoints)
        new_records.write.format("delta").mode("append").partitionBy("source_system","ingestion_date").save(silver_main_path)
        conflicted.write.format("delta").mode("append").partitionBy("source_system","ingestion_date").save(silver_conflicted_path)

        clean_count, conflicted_count = new_records.count(), conflicted.count()

        lineage_records.append((run_id, src["name"], bronze_path, silver_main_path, datetime.now(),
                                initial_count, clean_count, conflicted_count, compared_count, env["environment"], load_type))

        return clean_count, conflicted_count, compared_count

    except Exception as e:
        error_records.append((run_id, src["name"], str(e), datetime.now(), env["environment"]))
        failed_records.append(df)  # persist failed rows
        return 0, 0, 0

# -------------------------------------------------------------
# Main Job
# -------------------------------------------------------------
def main():
    run_id = str(uuid.uuid4())
    job_name = "Silver-Layer"
    job_start = datetime.now()

    total_clean, total_conflicted, total_compared = 0, 0, 0
    lineage_records, error_records, failed_records = [], [], []

    for src in sources:
        clean, conflicted, compared = process_source_to_silver(src, run_id, lineage_records, error_records, failed_records)
        total_clean += clean
        total_conflicted += conflicted
        total_compared += compared

    job_end = datetime.now()
    job_duration = (job_end - job_start).total_seconds()

    # ---------------- Audit Runs ----------------
    audit_schema = StructType([
        StructField("run_id", StringType(), True),
        StructField("job_name", StringType(), True),
        StructField("start_time", TimestampType(), True),
        StructField("end_time", TimestampType(), True),
        StructField("duration", DoubleType(), True),
        StructField("records_clean", LongType(), True),
        StructField("records_conflicted", LongType(), True),
        StructField("records_compared", LongType(), True),
        StructField("environment", StringType(), True)
    ])
    audit_data = [(run_id, job_name, job_start, job_end, job_duration,
                   total_clean, total_conflicted, total_compared, env["environment"])]
    audit_df = spark.createDataFrame(audit_data, schema=audit_schema)
    audit_df.write.format("delta").mode("append").option("mergeSchema","true").save(paths["audit_runs"])

        # ---------------- Audit Errors ----------------
    error_schema = StructType([
        StructField("run_id", StringType(), True),
        StructField("source_system", StringType(), True),
        StructField("error_message", StringType(), True),
        StructField("error_time", TimestampType(), True),
        StructField("environment", StringType(), True)
    ])
    error_df = spark.createDataFrame(error_records, schema=error_schema) if error_records else spark.createDataFrame([], schema=error_schema)
    error_df.write.format("delta").mode("append").option("mergeSchema","true").save(paths["audit_errors"])

    # ---------------- Audit Lineage ----------------
    lineage_schema = StructType([
        StructField("run_id", StringType(), True),
        StructField("source_system", StringType(), True),
        StructField("source_path", StringType(), True),
        StructField("target_path", StringType(), True),
        StructField("lineage_time", TimestampType(), True),
        StructField("bronze_count", LongType(), True),
        StructField("silver_clean_count", LongType(), True),
        StructField("silver_conflicted_count", LongType(), True),
        StructField("records_compared", LongType(), True),
        StructField("environment", StringType(), True),
        StructField("load_type", StringType(), True)   # FULL or INCREMENTAL
    ])
    lineage_df = spark.createDataFrame(lineage_records, schema=lineage_schema) if lineage_records else spark.createDataFrame([], schema=lineage_schema)
    lineage_df.write.format("delta").mode("append").option("mergeSchema","true").save(paths["audit_lineage"])

    # ---------------- Silver Error ----------------
    silver_error_schema = StructType([
        StructField("run_id", StringType(), True),
        StructField("source_system", StringType(), True),
        StructField("error_message", StringType(), True),
        StructField("error_time", TimestampType(), True),
        StructField("environment", StringType(), True)
    ])

    if failed_records:
        failed_union = failed_records[0]
        for fr in failed_records[1:]:
            failed_union = failed_union.unionByName(fr, allowMissingColumns=True)
        failed_union.write.format("delta").mode("append").option("mergeSchema","true").save(paths["silver_error"])
    else:
        # Ensure silver_error folder always exists
        try:
            spark.read.format("delta").load(paths["silver_error"])
        except AnalysisException:
            empty_df = spark.createDataFrame([], silver_error_schema)
            empty_df.write.format("delta").mode("overwrite").save(paths["silver_error"])

    print(f"Silver Layer complete. Run ID={run_id}, Clean={total_clean}, Conflicted={total_conflicted}, Compared={total_compared}, Duration={job_duration:.2f}s")

    # ---------------- Validation Outputs ----------------
    print("Displaying full audit tables:")

    display(spark.read.format("delta").load(paths["audit_runs"]))
    display(spark.read.format("delta").load(paths["audit_errors"]))
    display(spark.read.format("delta").load(paths["audit_lineage"]))
    display(spark.read.format("delta").load(paths["silver_error"]))

# -------------------------------------------------------------
# Entry Point
# -------------------------------------------------------------
main()
