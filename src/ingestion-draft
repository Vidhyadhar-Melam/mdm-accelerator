# Raw Ingestion Script (Databricks, Config-Driven, Delta Lake, Hybrid Full + Incremental)

import json, uuid
from datetime import datetime
from pyspark.sql.functions import lit, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, LongType
from delta.tables import DeltaTable

# -------------------------------------------------------------
# Load configs directly from S3 using dbutils.fs.head
# -------------------------------------------------------------
sources_path = "s3://databricks-amz-s3-bucket/mdm-accelerator/config/sources.json"
paths_path   = "s3://databricks-amz-s3-bucket/mdm-accelerator/config/paths.json"
env_path     = "s3://databricks-amz-s3-bucket/mdm-accelerator/config/environment.json"

sources = json.loads(dbutils.fs.head(sources_path, 100000))["sources"]
paths   = json.loads(dbutils.fs.head(paths_path, 100000))
env     = json.loads(dbutils.fs.head(env_path, 100000))

# -------------------------------------------------------------
# Helper: Ingest a source system (Full + Incremental)
# -------------------------------------------------------------
def ingest_source(src, run_id, error_records, lineage_records):
    try:
        start_time = datetime.now()

        # ✅ Read CSV directly from S3
        df_new = spark.read.option("header", "true") \
                           .option("inferSchema", "true") \
                           .option("delimiter", ",") \
                           .csv(src["path"])

        # Standardize column names
        for col_name in df_new.columns:
            lower = col_name.lower()
            if lower in ["cust_email","sf_email","erp_email","sap_email","email"]:
                df_new = df_new.withColumnRenamed(col_name,"email")
            if lower in ["cust_phone","phone_number","contact_phone","phone"]:
                df_new = df_new.withColumnRenamed(col_name,"phone")
            if lower in ["id","cust_id","customerid","customer_id"]:
                df_new = df_new.withColumnRenamed(col_name,"customer_id")

        # Add metadata
        df_new = (df_new.withColumn("source_system", lit(src["name"]))
                        .withColumn("ingestion_ts", current_timestamp())
                        .withColumn("run_id", lit(run_id)))

        raw_path = f"s3://databricks-amz-s3-bucket/mdm-accelerator/storage/raw/{src['name'].lower()}/{src['entity'].lower()}"

        # ✅ Hybrid Full + Incremental
        if not DeltaTable.isDeltaTable(spark, raw_path):
            # First run → Full load
            df_new.write.format("delta").mode("overwrite").save(raw_path)
            load_type = "FULL"
        else:
            # Subsequent runs → Incremental (MERGE using business_key from config)
            business_key = src.get("business_key", "customer_id")  # fallback if missing
            delta_table = DeltaTable.forPath(spark, raw_path)
            (
                delta_table.alias("t")
                .merge(df_new.alias("s"), f"t.{business_key} = s.{business_key}")
                .whenMatchedUpdateAll()
                .whenNotMatchedInsertAll()
                .execute()
            )
            load_type = "INCREMENTAL"

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # Add lineage record
        lineage_records.append((
            run_id, src["name"], src["path"], raw_path, start_time, end_time, env["environment"], load_type
        ))

        return int(df_new.count()), 0, "SUCCESS", start_time, end_time, float(duration), load_type

    except Exception as e:
        print(f"Error ingesting {src['name']}: {e}")
        error_records.append((
            run_id, src["name"], str(e), datetime.now(), env["environment"]
        ))
        return 0, 0, "FAILED", datetime.now(), datetime.now(), 0.0, "FAILED"

# -------------------------------------------------------------
# Main Job
# -------------------------------------------------------------
def main():
    run_id = str(uuid.uuid4())
    job_name = "Raw-Ingestion"
    job_start = datetime.now()

    total_loaded, total_rejected = 0, 0
    audit_records = []
    error_records = []
    lineage_records = []
    overall_status = "SUCCESS"

    for src in sources:
        loaded, rejected, status, start_time, end_time, duration, load_type = ingest_source(src, run_id, error_records, lineage_records)
        total_loaded += loaded
        total_rejected += rejected

        if status == "FAILED":
            overall_status = "FAILED"

        # Add per-source audit record
        audit_records.append((
            run_id, job_name, src["name"], start_time, end_time,
            float(duration), int(loaded), int(rejected), env["environment"], status, load_type
        ))

    job_end = datetime.now()
    job_duration = (job_end - job_start).total_seconds()

    # Add job-level summary record
    audit_records.append((
        run_id, job_name, "JOB_SUMMARY", job_start, job_end,
        float(job_duration), int(total_loaded), int(total_rejected),
        env["environment"], overall_status, "SUMMARY"
    ))

    # ✅ Schemas
    audit_schema = StructType([
        StructField("run_id", StringType(), True),
        StructField("job_name", StringType(), True),
        StructField("source_system", StringType(), True),
        StructField("start_time", TimestampType(), True),
        StructField("end_time", TimestampType(), True),
        StructField("duration", DoubleType(), True),
        StructField("records_loaded", LongType(), True),
        StructField("records_rejected", LongType(), True),
        StructField("environment", StringType(), True),
        StructField("status", StringType(), True),
        StructField("load_type", StringType(), True)   # NEW: FULL or INCREMENTAL
    ])

    error_schema = StructType([
        StructField("run_id", StringType(), True),
        StructField("source_system", StringType(), True),
        StructField("error_message", StringType(), True),
        StructField("error_time", TimestampType(), True),
        StructField("environment", StringType(), True)
    ])

    lineage_schema = StructType([
        StructField("run_id", StringType(), True),
        StructField("source_system", StringType(), True),
        StructField("source_path", StringType(), True),
        StructField("target_path", StringType(), True),
        StructField("start_time", TimestampType(), True),
        StructField("end_time", TimestampType(), True),
        StructField("environment", StringType(), True),
        StructField("load_type", StringType(), True)   # NEW: FULL or INCREMENTAL
    ])

    # ✅ Write Audit Runs
    audit_df = spark.createDataFrame(audit_records, schema=audit_schema)
    audit_df.write.format("delta").mode("append").option("mergeSchema","true").save(paths["audit_runs"])

    # ✅ Write Audit Errors
    error_df = spark.createDataFrame(error_records, schema=error_schema) if error_records else spark.createDataFrame([], schema=error_schema)
    error_df.write.format("delta").mode("append").option("mergeSchema","true").save(paths["audit_errors"])

    # ✅ Write Audit Lineage
    lineage_df = spark.createDataFrame(lineage_records, schema=lineage_schema) if lineage_records else spark.createDataFrame([], schema=lineage_schema)
    lineage_df.write.format("delta").mode("append").option("mergeSchema","true").save(paths["audit_lineage"])

    print(f"Ingestion complete. Run ID={run_id}, Loaded={total_loaded}, Rejected={total_rejected}, Status={overall_status}")

    # Validation
    display(spark.read.format("delta").load(paths["audit_runs"]))
    display(spark.read.format("delta").load(paths["audit_errors"]))
    display(spark.read.format("delta").load(paths["audit_lineage"]))

# -------------------------------------------------------------
# Entry Point
# -------------------------------------------------------------
main()
